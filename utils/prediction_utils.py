import os
import torch
import joblib
import numpy as np
from PIL import Image
from scipy.ndimage import label
from sklearn.cluster import KMeans
from utils.utils import normalize_array, image_patching, embedding_unpatching

def compute_mean_and_std(config):
    """
    Calculates the mean and standard deviation of pixel values from a list of images specified in the configuration object.

    Parameters
    ----------
    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - train_data_list (list): A list of image identifiers (usually filenames or relative paths) that specify the images to be processed.
        - ip (str): A string representing the path to the directory containing the images.

    Returns
    -------
    tuple
        A tuple containing two elements:
        - mean (float): The mean pixel value of the images.
        - std (float): The standard deviation of the pixel values of the images.
    """
    x = []
    for image_id in config.train_data_list:
        #Load image-target and append
        x_temp = np.array(Image.open(f'{config.ip}{image_id}'))/255
        x += list(x_temp)
    x = np.array(x)
    return x.mean(), x.std()

def compute_image_latent_embeddings(dataset, config, model, predict=False, low_pass=False, resize=False):
    """
    Function that computes latent embeddings for a set of images using a specified model and configuration.

    Parameters
    ----------
    dataset : list
        List of image identifiers (usually filenames or relative paths) that specify the images to be processed.

    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - ip (str): Path to the directory containing the images.
        - initial_img_patches_num (int): Number of initial image patches.
        - img_size (int): Size of the image.
        - mean (float): Mean pixel value for normalization.
        - std (float): Standard deviation of pixel values for normalization.
        - device (str or torch.device): Device to perform the computations on (e.g., 'cpu' or 'cuda').
        - patches_num (int): Number of patches along one dimension.
        - embed_dim (int): Dimensionality of the embedding.

    model : object
        The model used to infer latent embeddings from image patches. It must have a method `infer_latent` that accepts a tensor and returns an embedding.

    predict : bool, optional
        If True, the function will return unpatched embeddings. Default is False.

    low_pass : bool, optional
        If True, a low-pass filter is applied to the image before further processing. Default is False.

    resize : bool or tuple, optional
        If a value is provided, images will be resized to the specified dimensions before processing. Default is False.

    Returns
    -------
    np.ndarray
        An array of latent embeddings for the processed images.
    """
    x = []
    for image_id in dataset:
        #Load image-target and append
        x_temp = Image.open(f'{config.ip}{image_id}')
        if resize:
            x_temp = x_temp.resize((resize,resize), Image.BILINEAR)
            x_temp = normalize_array(np.array(x_temp))
        else:
            x_temp = np.array(x_temp)/255
        if low_pass:
            x_temp = normalize_array(low_pass_filter_image(x_temp,20))
        #Split Image
        x_temp = image_patching(x_temp, config.initial_img_patches_num, config.img_size)[0,0]
        x_temp = np.array(x_temp)
        #Normalize
        x_temp = (x_temp-config.mean)/config.std
        with torch.no_grad():
            x_temp = model.infer_latent(torch.from_numpy(x_temp[:,None,:,:]).float().to(config.device))[:,1:,:]
            x_temp = x_temp.detach().cpu().numpy()
            if predict:
                x += list(embedding_unpatching(x_temp.reshape(config.initial_img_patches_num,config.patches_num,config.patches_num,config.embed_dim)))
            else:
                x += list(x_temp)
    return np.array(x)

def compute_kmeans_on_training_set(config, model, cl=4):
    """
    Computes KMeans clustering on the training set using the latent embeddings generated by the given model.

    Parameters
    ----------
    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - train_data_list (list): A list of image identifiers (usually filenames or relative paths) specifying the images in the training set.

    model : object
        The model used to infer latent embeddings from image patches.

    cl : int, optional
        The number of clusters to be used in KMeans clustering. Default is 4.

    Returns
    -------
    None
    """
    # Compute latent embeddings on the training set
    x = compute_image_latent_embeddings(config.train_data_list, config, model)
    flat = x.shape[0] * x.shape[1]
    x = x.reshape(flat, -1)
    # Compute KMeans
    kmeans = KMeans(cl, random_state=0, n_init="auto").fit(x)
    p = kmeans.predict(x)
    sums = []
    for i in range(cl):
        p_ = np.zeros(p.shape)
        p_[np.where(p == i)] = 1
        sums.append(p_.sum())
    # Save the model to a file
    joblib.dump(kmeans, kmeans_path)

def find_particles_cluster(kmeans, res, config, model, cl=4):
    """
    Finds the cluster number associated with particles in an image using a pre-trained KMeans model and image embeddings.

    Parameters
    ----------
    kmeans : object
        A pre-trained KMeans model used to predict cluster assignments for image embeddings.

    res : int
        The resolution of the image to be processed.

    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - ip (str): Path to the directory containing the images.
        - patch_size (int): Size of the patches to be used in the embedding process.
        - embed_dim (int): Dimensionality of the embeddings.

    model : object
        The model used to infer latent embeddings from image patches. It must have a method `infer_latent` that accepts a tensor and returns an embedding.

    cl : int, optional
        Cluster number to be used for processing. Default is 4.

    Returns
    -------
    int
        The cluster number associated with particles in the image.
    """
    temp_idp = config.ip
    config.ip = './params/'
    x = compute_image_latent_embeddings([f'input_example.png'], config, model, predict=True)
    x_c = np.array(Image.open(f'{config.ip}target_example.png')) / 255
    config.ip = temp_idp
    x = x.reshape(-1, config.embed_dim)
    x = (kmeans.predict(x) + 1).reshape(res // config.patch_size, res // config.patch_size)
    clst, clst_counts = np.unique(x_c * x, return_counts=True)
    # Return cluster number
    return int(clst[clst_counts.argsort()[-2]] - 1)

def window_embedding(image, image_emb, particle_diameter_512, config):
    """
    Applies a window-based smoothing to the latent embeddings of an image.

    Parameters
    ----------
    image : str
        Identifier (usually filename or relative path) of the image being processed.

    image_emb : np.ndarray
        Latent embeddings of the image, expected to be a 2D array with shape (H, W, embed_dim).

    particle_diameter_512 : dict
        Dictionary mapping image identifiers to particle diameters.

    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - embed_dim (int): Dimensionality of the embeddings.

    Returns
    -------
    np.ndarray
        A 2D array of the same shape as `image_emb` containing the smoothed latent embeddings.
    """
    try:
        pd = particle_diameter_512[image[:5]]
    except:
        pd = particle_diameter_512
    pd = int(pd // 8)
    w = 4
    if w > pd:
        w = 2
    w2 = w // 2
    image_emb_mean = np.zeros(image_emb.shape, dtype='float32')
    rng = range(w2, image_emb.shape[0] - w2)
    for i in rng:
        for j in rng:
            image_emb_temp = image_emb[i-w2:i+w2, j-w2:j+w2].reshape(-1, config.embed_dim)
            image_emb_mean[i, j] = np.mean(image_emb_temp, axis=0)
    return image_emb_mean

def find_your_cluster(kmeans, kmeans_cluster, image, config, cl):
    """
    Finds the cluster number associated with particles in an image using a hierarchical clustering approach.

    Parameters
    ----------
    kmeans : object
        A pre-trained KMeans model used for initial clustering.

    kmeans_cluster : int
        The cluster number to be identified in the hierarchical clustering process.

    image : np.ndarray
        Latent embeddings of the image, expected to be a 2D array with shape (H, W, embed_dim).

    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - embed_dim (int): Dimensionality of the embeddings.
        - output_image_len (int): Length of the output image.

    cl : int
        The number of clusters to be used in the hierarchical clustering process.

    Returns
    -------
    tuple
        A tuple containing three elements:
        - temp_pred (np.ndarray): Predicted cluster assignments after hierarchical clustering.
        - temp_kmeans (object): The KMeans model trained on the latent embeddings of the image.
        - your_cluster (int): The cluster number associated with particles in the image.
    """
    temp_kmeans = KMeans(cl, random_state=0, n_init="auto").fit(image.reshape(-1, config.embed_dim))
    temp_pred = temp_kmeans.predict(image.reshape(-1, config.embed_dim)).reshape(config.output_image_len, config.output_image_len)
    # Compute image cluster
    im_pred = kmeans.predict(image.reshape(-1, config.embed_dim)).reshape(config.output_image_len, config.output_image_len)
    pred_c = np.zeros(im_pred.shape)
    pred_c[np.where(im_pred == kmeans_cluster)] = 1
    # Find your cluster
    prmtr = int(config.output_image_len / 10)
    p = slice(prmtr, config.output_image_len - prmtr)
    pclst, pclst_count = np.unique(temp_pred, return_counts=True)
    clst, clst_counts = np.unique((pred_c * (temp_pred + 1))[p, p], return_counts=True)
    if len(clst_counts) == 1:
        clst, clst_counts = np.unique((pred_c * (temp_pred + 1)), return_counts=True)
    try:
        your_clst_argsort = clst_counts.argsort()[-2]
    except IndexError:
        return im_pred, kmeans, kmeans_cluster
    your_clst_temp = int(clst[your_clst_argsort] - 1)
    try:
        poss_clst_argsort = clst_counts.argsort()[-3]
        poss_clst = int(clst[poss_clst_argsort] - 1)
        if pclst_count[your_clst_temp] / clst_counts[your_clst_argsort] > pclst_count[poss_clst] / clst_counts[
            poss_clst_argsort]:
            your_clst_temp = poss_clst
    except IndexError:
        pass
    return temp_pred, temp_kmeans, your_clst_temp


def predict_particles_maps(prediction_set, temp_embeddings_path, kmeans, your_cluster, config, particle_diameter_512):
    """
    Predicts particle maps from a set of latent embeddings using hierarchical clustering.

    Parameters
    ----------
    prediction_set : list
        List of image identifiers (usually filenames or relative paths) to be processed for predictions.

    temp_embeddings_path : str
        Path to the directory containing the latent embeddings of the images.

    kmeans : object
        A pre-trained KMeans model used for initial clustering.

    your_cluster : int
        The cluster number to be identified in the hierarchical clustering process.

    config : object
        An object containing the configuration for the function. It must have the following attributes:
        - pde (str): Description of the experiment, used for naming the results directory.

    particle_diameter_512 : dict
        Dictionary mapping image identifiers to particle diameters.

    Returns
    -------
    np.ndarray
        An array containing the predicted particle maps for the images.
    """
    # Define prediction maps list
    prediction_maps = []
    # For each latent prediction in prediction set
    for image_i, image in enumerate(prediction_set):
        # Load prediction
        image_emb = np.load(f'{temp_embeddings_path}{".".join(image.split(".")[:-1])}.npy')
        # Apply smoothing at the level of embedding
        image_emb = window_embedding(image, image_emb, particle_diameter_512, config)
        # Set training kmeans as the first kmeans
        kmeans_, kmeans_cluster = kmeans, your_cluster
        # Apply the hierarchical clustering on the kmeans of the micrograph embedding level
        for i in (3, 4, 5):
            temp_pred, kmeans_, kmeans_cluster = find_your_cluster(kmeans_, kmeans_cluster, image_emb, config, i)
        prediction_cluster = np.zeros(temp_pred.shape)
        prediction_cluster[np.where(temp_pred == kmeans_cluster)] = 1
        prediction_maps.append(prediction_cluster)
        print(image_i, end='\r')
    print(f'{image_i+1} particle maps have been predicted.')
    prediction_maps = np.array(prediction_maps)
    # Save mask
    path_to_save_maps = f'./results/predicted_cluster_images/kmean_5_{config.pde}/'
    if not os.path.exists(path_to_save_maps):
        os.mkdir(path_to_save_maps)
    for i, ii in enumerate(prediction_maps):
        Image.fromarray(ii * 255).convert('L').save(f'{path_to_save_maps}{prediction_set[i]}')
    return prediction_maps

def components_to_coordinates(label_where, arr):
    """
    Converts the labeled coordinates to weighted coordinates based on the input array.

    Parameters
    ----------
    label_where : tuple
        Tuple containing arrays of coordinates corresponding to labeled components.

    arr : np.ndarray
        Array containing the values corresponding to the labeled components.

    Returns
    -------
    tuple
        A tuple containing two elements:
        - weighted_coords (np.ndarray): Weighted coordinates computed from the labeled coordinates.
        - weight_sum (float): Sum of weights associated with the labeled coordinates.
    """
    label_coords, label_coords_weights = np.array(label_where).T.astype(float), arr[label_where]
    weight_sum = np.sum(label_coords_weights)
    # Compute the weighted coordinates
    weighted_coords = label_coords * np.expand_dims(label_coords_weights / weight_sum, axis=1)
    return weighted_coords, weight_sum

def coordinates_to_dmatrix(a_coords, b_coords):
    '''
    Creates distance matrix for numpy.array input.

    Parameters
    ----------
    a_coords : numpy.array
        numpy.array of shape (N,3) that contains coordinates information.

    b_coords : numpy.array
        numpy.array of shape (M,3) that contains coordinates information.

    Returns
    -------
    numpy.array of shape (N,M), the distance matrix of a_coords and b_coords.
    '''
    a, b = torch.from_numpy(a_coords), torch.from_numpy(b_coords)
    return np.array(torch.cdist(a,b))

def remove_duplicates(predicted_coords, predicted_weights, dist, batch_size = 50):
    '''
    Function that removes duplicates simply, created for post processing of the 3D Unet, removes nearby water coordinates based on their scores.

    Parameters
    ----------
    predicted_coords : numpy.array
        numpy array of shape (N,3) of the predicted water coordinates.

    predicted_weights : numpy.array
        numpy array of shape (N,1) with the corresponding prediction scores for the predicted_coords.

    dist : float
        distance value to look after for duplicates.

    batch_size : int
        batch size value to process for duplicates, default 50.

    Returns
    -------
    todelete : numpy.array
        indexes of predicted water coordinates to delete as duplicates.
    '''
    #Make a copy of your predicted_coords and predicted_weights
    predicted_coords_refined, predicted_weights_refined = np.copy(predicted_coords), np.copy(predicted_weights)
    coords_len = len(predicted_coords)
    #Find the batches according to batch size
    coords_batches = int(np.ceil(coords_len/batch_size))+1
    #Compute batch loops for a and b coordinates
    batch_loop = np.linspace(0, coords_len, coords_batches, dtype=int)
    #todelete list will keep indexes you want to remove
    todelete = []
    #Start computing for each batch
    for b_i, batch in enumerate(batch_loop[:-1]):
        indexes = slice(batch, batch_loop[b_i+1])
        predicted_coords_batch = predicted_coords[indexes]
        #Calculate distances of predicted waters
        d_matrix = coordinates_to_dmatrix(predicted_coords_batch, predicted_coords)
        #Find duplicates within certain distance
        duplicates = np.unique(np.where(d_matrix<dist)[0])
        #Sort duplicates by the larger predicted weight index to the smallest predicted weight index.
        #duplicates = duplicates[np.argsort(-predicted_weights[duplicates])]
        #Keep the original indexing of duplicates
        duplicates_original_indexing = duplicates+batch
        #For each duplicate
        for i,j in zip(duplicates,duplicates_original_indexing):
            #Keep indexes of close predicted waters
            closeby = np.where(d_matrix[i]<dist)[0]
            #Remove j from closeby
            closeby_r = np.delete(closeby, np.argwhere(closeby==j)[0])
            #If your coordinate has the higher prediction in the region
            if (predicted_weights[j] > predicted_weights[closeby_r]).all():
                todelete += closeby_r.tolist()
            else:
                todelete.append(j)
    return np.unique(todelete)

def write_star_file(star_file_name, data_particles):
    """
    Writes a STAR (Self-Defining Text Archiving and Retrieval) file with a specific header and particle data.

    Parameters
    ----------
    star_file_name : str
        The name of the STAR file to be created, including its path if necessary.
    data_particles : list
        A list of strings, where each string corresponds to a row of particle data to be written to the file. Each row typically represents
        a particle's metadata, such as coordinates and associated micrograph.

    File Format
    -----------
    The function writes the file in the following format:
    - A predefined header specifying the data type and column names:
        ```
        data_particles

        loop_
        _rlnMicrographName #1
        _rlnCoordinateX #2
        _rlnCoordinateY #3
        ```
    - Followed by the rows of `data_particles`.

    Example
    -------
    ```python
    star_file_name = "output.star"
    data_particles = [
        "micrograph_001.mrc 1234.56 789.10",
        "micrograph_002.mrc 2234.56 1789.10"
    ]
    write_star_file(star_file_name, data_particles)
    ```
    This would create a `output.star` file with the predefined header followed by the data rows provided in `data_particles`.

    Notes
    -----
    - This function assumes the input list `data_particles` contains properly formatted strings.
    - Any existing file with the same name as `star_file_name` will be overwritten.
    """
    data_particles_columns = ['',
                              'data_particles',
                              '',
                              'loop_',
                              '_rlnMicrographName #1 ',
                              '_rlnCoordinateX #2 ',
                              '_rlnCoordinateY #3 ',]
    #Open and write the star file
    with open(star_file_name, 'w') as f:
        for line in data_particles_columns:
            f.write(f'{line}\n')
        for line in data_particles:
            f.write(f'{line}\n')

def pick_particles(config, pred_c, v_ids, experiment, dset_name, res, cap_values, particle_diameter_512, path_to_save=None):
    """
    Picks particles from the predicted cluster masks using post-processing techniques.

    Parameters
    ----------
    config : object
        An object containing variable configurations for the prediction.

    pred_c : np.ndarray
        Array containing predicted cluster masks for the images.

    v_ids : list
        List of image identifiers (usually filenames or relative paths) for the images being processed.

    experiment : str
        Description of the experiment.

    dset_name : str
        Name of the dataset.

    res : int
        Resolution of the images.

    cap_values : list
        List of values used to cap the cluster masks.

    path_to_save : str, optional
        Path to the directory where the picked particles will be saved. Default is None.

    Returns
    -------
    None
    """
    star_file = []
    for im_i, im_arr in enumerate(pred_c[:]):
        try:
            k_ = particle_diameter_512[v_ids[im_i][:5]]
        except:
            k_ = particle_diameter_512
        p_diameter_cubed = k_ ** 2
        k = int(k_)
        if k % 2:
            k += 1
        k2 = int(k // 2)
        if not path_to_save:
            path_to_save = f'./results/prediction_{experiment}_{dset_name}_npy_{res}/'
        if not os.path.exists(path_to_save):
            os.mkdir(path_to_save)
        ### Define and apply convolution
        im_torch = torch.from_numpy(im_arr)  # /255
        torch_kernel_ones = torch.ones((1, 1, 3, 3))
        im_torch_conv = torch.nn.functional.conv2d(im_torch[None, None, :, :].float(), torch_kernel_ones,
                                                    padding=torch_kernel_ones.shape[-1] // 2)
        ##
        im_arr_conv = np.array(im_torch_conv)[0, 0]
        if im_arr_conv.max():
            im_arr_conv /= im_arr_conv.max()
        im_conv = Image.fromarray(im_arr_conv * 255)
        im_conv_resized = im_conv.resize((512, 512), Image.BILINEAR).convert('L')
        im_conv_resized_arr_ = np.array(im_conv_resized) / 255
        # im_arr_percentage = im_conv_resized_arr_.sum()/len(im_conv_resized_arr_)**2
        labels_coords, coords_weight = [], []
        for c in cap_values:
            im_conv_resized_arr = (im_conv_resized_arr_ > c).astype(float)
            if im_conv_resized_arr.sum() / len(im_conv_resized_arr) ** 2 <= 0.04:
                continue
            else:
                labeled, ncomponents = label(im_conv_resized_arr)
                for i in range(1, ncomponents + 1):
                    label_where = np.where(labeled == i)
                    pixels_num = len(label_where[0])
                    weighted_coords, weight_sum = components_to_coordinates(label_where, im_conv_resized_arr_)
                    label_coords, label_coords_weights = np.array(label_where).T.astype(float), im_conv_resized_arr_[
                        label_where]
                    ###Assign coordinates and their weights
                    labels_coords.append(np.sum(weighted_coords, axis=0))
                    coords_weight.append(len(label_coords_weights))
                ### remove duplicates
                to_delete = remove_duplicates(np.array(labels_coords), np.array(coords_weight), k_ / 2)
                if len(to_delete):
                    labels_coords, coords_weight = np.delete(labels_coords, to_delete, axis=0), np.delete(coords_weight, to_delete, axis=0)
                break
        labels_coords_, coords_weight_ = np.array(labels_coords), np.array(coords_weight)
        to_delete = np.where(((labels_coords_ - k_ / 1.5) < 0) | ((labels_coords_ + k_ / 1.5) > 512))[0]
        if len(to_delete):
            labels_coords_, coords_weight_ = np.delete(labels_coords_, to_delete, axis=0), np.delete(coords_weight_, to_delete, axis=0)
        im_out = []
        for lci, lc in enumerate(labels_coords_):
            x_i, y_i = np.round(lc).astype(int)
            im_out.append([x_i, y_i, k2, coords_weight_[lci]])
            #Keep info for star file
            #l,c = config.mshape[0]-(2*float(x_i)*config.recover_resize_coeff[0]), 2*float(y_i)*config.recover_resize_coeff[1]
            l,c = 2*float(y_i)*config.recover_resize_coeff[1], config.mshape[0]-(2*float(x_i)*config.recover_resize_coeff[0])
            micrograph_name = ".".join(v_ids[im_i].split('.')[:-1])
            star_file.append(f"{micrograph_name}.mrc {l} {c}")
            ###
        np.save(f"{path_to_save}{micrograph_name}.npy", np.array(im_out))
        print(im_i+1, end='\r')
    #Write star file
    write_star_file(f'./results/star_files/prediction_{dset_name}.star', star_file)

#Particle diameter when original micrograph has been resized at (512,512)
particle_diameter_512 = {'10028': 28.0,'10081': 21.252830188679244,'10590': 21.804851752021563,'10096': 11.592452830188678,\
                         '10760': 14.628571428571428,'10240': 21.528840970350405,'10406': 29.257142857142856,'10669': 50.37196765498652,\
                         '10289': 22.35687331536388,'10532': 21.75,'10077': 27.0,'10737': 22.396871945259043,'10017': 13.5,\
                         '10291': 17.940700808625337,'10061': 32.50026954177898,'10059': 18.216711590296494,'11183': 19.89442815249267,\
                         '10184': 16.28463611859838,'10816': 24.771967654986522,'10075': 29.125,'10444': 29.15347018572825,\
                         '11056': 29.15347018572825,'10671': 29.15347018572825,'11057': 29.15347018572825,'10576': 16.077628032345014}
